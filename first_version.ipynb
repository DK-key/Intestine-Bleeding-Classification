{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter all positive images out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all positive #1 (活动性出血) images -- 429 total images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob\n",
    "pos_ind_df = pd.read_excel(r'C:\\Users\\wiqwe\\Desktop\\上交科研\\c活动性出血.xlsx')\n",
    "pos_ind_df.columns\n",
    "name_ind = pos_ind_df[[' 姓名','出血典型病变图片号码']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traverse all positive image folders\n",
    "import re\n",
    "folder_prefix = r'C:\\Users\\wiqwe\\Desktop\\上交科研\\阳性'\n",
    "folders = glob.glob(os.path.join(folder_prefix,'*'))\n",
    "# print(folders)\n",
    "name_to_folder = {}\n",
    "\n",
    "def name_extract(folder):\n",
    "    pattern = re.compile('\\d{6,10}(.+)')\n",
    "    name = pattern.findall(folder)\n",
    "    name_to_folder[name[0]] = folder\n",
    "    return name\n",
    "\n",
    "names = [name_extract(i) for i in folders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy selected images from patient's folder to dataset folder\n",
    "import shutil\n",
    "pos_img_index = 0\n",
    "POSITIVE_IMGS_FOLDER = r'C:\\Users\\wiqwe\\Desktop\\上交科研\\selected_positive_#1_imgs'\n",
    "def func(a):\n",
    "    name, img_inds = a.tolist()\n",
    "    if name.endswith('2'): name = name[:-1]\n",
    "#     print(name, img_inds,'|', type(img_inds), len(img_inds), 'called')\n",
    "    img_inds = img_inds.replace(' ', '').replace('，', ',').strip(',').replace('.',',').split(',')\n",
    "    name = name.strip()\n",
    "#     print(type(img_inds), img_inds)\n",
    "    \n",
    "    def copy_file_func(img_ind):\n",
    "        global pos_img_index\n",
    "        full_path = os.path.join(name_to_folder[name], img_ind + '.jpg')\n",
    "        print(full_path)\n",
    "        paste_path = os.path.join(POSITIVE_IMGS_FOLDER, str(pos_img_index) + '.jpg')\n",
    "        shutil.copyfile(full_path,paste_path)\n",
    "        pos_img_index += 1\n",
    "   \n",
    "    _ = [copy_file_func(i) for i in img_inds]\n",
    "#     print(name, img_inds)\n",
    "    \n",
    "_ = name_ind.apply(func, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([' 姓名', ' 检查日期', '工程师加密', ' 内镜诊断', 'Unnamed: 4', '病变图片编号', '其他'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all positive #2 (血管畸形) images -- 429 total images\n",
    "pos_ind_df = pd.read_excel(r'C:\\Users\\wiqwe\\Desktop\\上交科研\\消化道出血病人表-血管畸形80.xlsx')\n",
    "pos_ind_df.columns\n",
    "# name_ind = pos_ind_df[[' 姓名','病变图片编号']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get negative images -- 454\n",
    "import random\n",
    "NEGATIVE_IMGS_FOLDER = r'C:\\Users\\wiqwe\\Desktop\\上交科研\\selected_negative_imgs'\n",
    "folder_prefix = r'C:\\Users\\wiqwe\\Desktop\\上交科研\\阴性'\n",
    "neg_img_index = 0\n",
    "folders = glob.glob(os.path.join(folder_prefix,'*'))\n",
    "for folder in folders:\n",
    "    images = glob.glob(os.path.join(folder, '*'))\n",
    "    if not images: continue\n",
    "    image = random.choice(images)\n",
    "    image_basename = os.path.basename(image)\n",
    "    paste_path = os.path.join(NEGATIVE_IMGS_FOLDER, str(neg_img_index) + '.jpg')\n",
    "    neg_img_index += 1\n",
    "#     print(image)\n",
    "    shutil.copyfile(image,paste_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429 (224, 224, 3)\n",
      "883 (224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# image preprocessing pipeline (add positive and negative images)\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import glob, os\n",
    "BLEEDING_IMGS_FOLDER = r'C:\\Users\\wiqwe\\Desktop\\上交科研\\selected_bleeding_imgs'\n",
    "NEGATIVE_IMGS_FOLDER = r'C:\\Users\\wiqwe\\Desktop\\上交科研\\selected_negative_imgs'\n",
    "\n",
    "train_data = []\n",
    "train_label = []\n",
    "validation_data = []\n",
    "validation_label = []\n",
    "for img in glob.glob(os.path.join(POSITIVE_IMGS_FOLDER ,'*')):\n",
    "    im = PIL.Image.open(img)\n",
    "    im = im.resize((224,224))\n",
    "    im = np.array(im)\n",
    "    im = (im / 255.0) - 1.0\n",
    "#     im_roll = np.rollaxis(im, 2, 0)\n",
    "#     assert (im[:,:,0] == im_roll[0,:,:]).all()\n",
    "#     assert (im[:,:,1] == im_roll[1,:,:]).all()\n",
    "#     assert (im[:,:,2] == im_roll[2,:,:]).all()\n",
    "    train_data.append(np.array(im))\n",
    "print(len(train_data), train_data[0].shape)\n",
    "train_label += [[0, 1] for i in range(len(train_data))]\n",
    "\n",
    "for img in glob.glob(os.path.join(NEGATIVE_IMGS_FOLDER ,'*')):\n",
    "    im = PIL.Image.open(img)\n",
    "    im = im.resize((224,224))\n",
    "    im = np.array(im)\n",
    "    im = (im / 255.0) - 1.0\n",
    "#     im_roll = np.rollaxis(im, 2, 0)\n",
    "#     assert (im[:,:,0] == im_roll[0,:,:]).all()\n",
    "#     assert (im[:,:,1] == im_roll[1,:,:]).all()\n",
    "#     assert (im[:,:,2] == im_roll[2,:,:]).all()\n",
    "    train_data.append(np.array(im))\n",
    "print(len(train_data), train_data[0].shape)\n",
    "train_label += [[1, 0] for i in range(len(train_data) - len(train_label))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "recovered_img = ((train_data[0]+1)*255).astype(np.uint8)\n",
    "print(recovered_img.shape)\n",
    "PIL.Image.fromarray(recovered_img).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data to be train and validation set\n",
    "import random\n",
    "VALIDATION_RATIO = 0.1\n",
    "num_validation = int(len(train_data) * VALIDATION_RATIO)\n",
    "validation_indices = random.sample(list(range(len(train_data))))\n",
    "validation_data = train_data[validation_indices]\n",
    "validation_label = train_label[validation_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.stack(train_data, axis=0)\n",
    "train_label = np.array(train_label)\n",
    "print(train_data.shape, train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_50 = ResnetBuilder.build_resnet_50((3,224,224), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.001, amsgrad=True, decay=0.00001)\n",
    "resnet_50.compile(optimizer, loss=keras.losses.categorical_crossentropy, metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 794 samples, validate on 89 samples\n",
      "Epoch 1/10\n",
      "794/794 [==============================] - 146s 184ms/step - loss: 5.6880 - categorical_accuracy: 0.7557 - val_loss: 9.3965 - val_categorical_accuracy: 0.0674\n",
      "Epoch 2/10\n",
      "794/794 [==============================] - 16s 21ms/step - loss: 4.7413 - categorical_accuracy: 0.8904 - val_loss: 20.2437 - val_categorical_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "794/794 [==============================] - 16s 20ms/step - loss: 3.9602 - categorical_accuracy: 0.9610 - val_loss: 12.5301 - val_categorical_accuracy: 0.0112\n",
      "Epoch 4/10\n",
      "794/794 [==============================] - 16s 20ms/step - loss: 3.4109 - categorical_accuracy: 0.9597 - val_loss: 12.9912 - val_categorical_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "794/794 [==============================] - 16s 20ms/step - loss: 2.9604 - categorical_accuracy: 0.9572 - val_loss: 7.4522 - val_categorical_accuracy: 0.0787\n",
      "Epoch 6/10\n",
      "794/794 [==============================] - 16s 20ms/step - loss: 2.6048 - categorical_accuracy: 0.9534 - val_loss: 18.4586 - val_categorical_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "794/794 [==============================] - 18s 23ms/step - loss: 2.2838 - categorical_accuracy: 0.9710 - val_loss: 3.5178 - val_categorical_accuracy: 0.5393\n",
      "Epoch 8/10\n",
      "794/794 [==============================] - 16s 20ms/step - loss: 2.0688 - categorical_accuracy: 0.9698 - val_loss: 6.4770 - val_categorical_accuracy: 0.1573\n",
      "Epoch 9/10\n",
      "794/794 [==============================] - 17s 21ms/step - loss: 1.8024 - categorical_accuracy: 0.9962 - val_loss: 1.9405 - val_categorical_accuracy: 0.9213\n",
      "Epoch 10/10\n",
      "794/794 [==============================] - 17s 22ms/step - loss: 1.6359 - categorical_accuracy: 0.9924 - val_loss: 2.3380 - val_categorical_accuracy: 0.6180\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2200f5b8c88>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_50.fit(train_data, train_label, epochs=10, batch_size=32, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras resnet definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import six\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import (\n",
    "\n",
    "    Input,\n",
    "\n",
    "    Activation,\n",
    "\n",
    "    Dense,\n",
    "\n",
    "    Flatten\n",
    "\n",
    ")\n",
    "\n",
    "from keras.layers.convolutional import (\n",
    "\n",
    "    Conv2D,\n",
    "\n",
    "    MaxPooling2D,\n",
    "\n",
    "    AveragePooling2D\n",
    "\n",
    ")\n",
    "\n",
    "from keras.layers.merge import add\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _bn_relu(input):\n",
    "\n",
    "    \"\"\"Helper to build a BN -> relu block\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    norm = BatchNormalization(axis=CHANNEL_AXIS)(input)\n",
    "\n",
    "    return Activation(\"relu\")(norm)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _conv_bn_relu(**conv_params):\n",
    "\n",
    "    \"\"\"Helper to build a conv -> BN -> relu block\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    filters = conv_params[\"filters\"]\n",
    "\n",
    "    kernel_size = conv_params[\"kernel_size\"]\n",
    "\n",
    "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
    "\n",
    "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
    "\n",
    "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
    "\n",
    "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
    "\n",
    "\n",
    "\n",
    "    def f(input):\n",
    "\n",
    "        conv = Conv2D(filters=filters, kernel_size=kernel_size,\n",
    "\n",
    "                      strides=strides, padding=padding,\n",
    "\n",
    "                      kernel_initializer=kernel_initializer,\n",
    "\n",
    "                      kernel_regularizer=kernel_regularizer)(input)\n",
    "\n",
    "        return _bn_relu(conv)\n",
    "\n",
    "\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _bn_relu_conv(**conv_params):\n",
    "\n",
    "    \"\"\"Helper to build a BN -> relu -> conv block.\n",
    "\n",
    "    This is an improved scheme proposed in http://arxiv.org/pdf/1603.05027v2.pdf\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    filters = conv_params[\"filters\"]\n",
    "\n",
    "    kernel_size = conv_params[\"kernel_size\"]\n",
    "\n",
    "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
    "\n",
    "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
    "\n",
    "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
    "\n",
    "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
    "\n",
    "\n",
    "\n",
    "    def f(input):\n",
    "\n",
    "        activation = _bn_relu(input)\n",
    "\n",
    "        return Conv2D(filters=filters, kernel_size=kernel_size,\n",
    "\n",
    "                      strides=strides, padding=padding,\n",
    "\n",
    "                      kernel_initializer=kernel_initializer,\n",
    "\n",
    "                      kernel_regularizer=kernel_regularizer)(activation)\n",
    "\n",
    "\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _shortcut(input, residual):\n",
    "\n",
    "    \"\"\"Adds a shortcut between input and residual block and merges them with \"sum\"\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Expand channels of shortcut to match residual.\n",
    "\n",
    "    # Stride appropriately to match residual (width, height)\n",
    "\n",
    "    # Should be int if network architecture is correctly configured.\n",
    "\n",
    "    input_shape = K.int_shape(input)\n",
    "\n",
    "    residual_shape = K.int_shape(residual)\n",
    "\n",
    "    stride_width = int(round(input_shape[ROW_AXIS] / residual_shape[ROW_AXIS]))\n",
    "\n",
    "    stride_height = int(round(input_shape[COL_AXIS] / residual_shape[COL_AXIS]))\n",
    "\n",
    "    equal_channels = input_shape[CHANNEL_AXIS] == residual_shape[CHANNEL_AXIS]\n",
    "\n",
    "\n",
    "\n",
    "    shortcut = input\n",
    "\n",
    "    # 1 X 1 conv if shape is different. Else identity.\n",
    "\n",
    "    if stride_width > 1 or stride_height > 1 or not equal_channels:\n",
    "\n",
    "        shortcut = Conv2D(filters=residual_shape[CHANNEL_AXIS],\n",
    "\n",
    "                          kernel_size=(1, 1),\n",
    "\n",
    "                          strides=(stride_width, stride_height),\n",
    "\n",
    "                          padding=\"valid\",\n",
    "\n",
    "                          kernel_initializer=\"he_normal\",\n",
    "\n",
    "                          kernel_regularizer=l2(0.0001))(input)\n",
    "\n",
    "\n",
    "\n",
    "    return add([shortcut, residual])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _residual_block(block_function, filters, repetitions, is_first_layer=False):\n",
    "\n",
    "    \"\"\"Builds a residual block with repeating bottleneck blocks.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def f(input):\n",
    "\n",
    "        for i in range(repetitions):\n",
    "\n",
    "            init_strides = (1, 1)\n",
    "\n",
    "            if i == 0 and not is_first_layer:\n",
    "\n",
    "                init_strides = (2, 2)\n",
    "\n",
    "            input = block_function(filters=filters, init_strides=init_strides,\n",
    "\n",
    "                                   is_first_block_of_first_layer=(is_first_layer and i == 0))(input)\n",
    "\n",
    "        return input\n",
    "\n",
    "\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def basic_block(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
    "\n",
    "    \"\"\"Basic 3 X 3 convolution blocks for use on resnets with layers <= 34.\n",
    "\n",
    "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def f(input):\n",
    "\n",
    "\n",
    "\n",
    "        if is_first_block_of_first_layer:\n",
    "\n",
    "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
    "\n",
    "            conv1 = Conv2D(filters=filters, kernel_size=(3, 3),\n",
    "\n",
    "                           strides=init_strides,\n",
    "\n",
    "                           padding=\"same\",\n",
    "\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "\n",
    "                           kernel_regularizer=l2(1e-4))(input)\n",
    "\n",
    "        else:\n",
    "\n",
    "            conv1 = _bn_relu_conv(filters=filters, kernel_size=(3, 3),\n",
    "\n",
    "                                  strides=init_strides)(input)\n",
    "\n",
    "\n",
    "\n",
    "        residual = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv1)\n",
    "\n",
    "        return _shortcut(input, residual)\n",
    "\n",
    "\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def bottleneck(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
    "\n",
    "    \"\"\"Bottleneck architecture for > 34 layer resnet.\n",
    "\n",
    "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
    "\n",
    "\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        A final conv layer of filters * 4\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def f(input):\n",
    "\n",
    "\n",
    "\n",
    "        if is_first_block_of_first_layer:\n",
    "\n",
    "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
    "\n",
    "            conv_1_1 = Conv2D(filters=filters, kernel_size=(1, 1),\n",
    "\n",
    "                              strides=init_strides,\n",
    "\n",
    "                              padding=\"same\",\n",
    "\n",
    "                              kernel_initializer=\"he_normal\",\n",
    "\n",
    "                              kernel_regularizer=l2(1e-4))(input)\n",
    "\n",
    "        else:\n",
    "\n",
    "            conv_1_1 = _bn_relu_conv(filters=filters, kernel_size=(1, 1),\n",
    "\n",
    "                                     strides=init_strides)(input)\n",
    "\n",
    "\n",
    "\n",
    "        conv_3_3 = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv_1_1)\n",
    "\n",
    "        residual = _bn_relu_conv(filters=filters * 4, kernel_size=(1, 1))(conv_3_3)\n",
    "\n",
    "        return _shortcut(input, residual)\n",
    "\n",
    "\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _handle_dim_ordering():\n",
    "\n",
    "    global ROW_AXIS\n",
    "\n",
    "    global COL_AXIS\n",
    "\n",
    "    global CHANNEL_AXIS\n",
    "\n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "\n",
    "        ROW_AXIS = 1\n",
    "\n",
    "        COL_AXIS = 2\n",
    "\n",
    "        CHANNEL_AXIS = 3\n",
    "\n",
    "    else:\n",
    "\n",
    "        CHANNEL_AXIS = 1\n",
    "\n",
    "        ROW_AXIS = 2\n",
    "\n",
    "        COL_AXIS = 3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _get_block(identifier):\n",
    "\n",
    "    if isinstance(identifier, six.string_types):\n",
    "\n",
    "        res = globals().get(identifier)\n",
    "\n",
    "        if not res:\n",
    "\n",
    "            raise ValueError('Invalid {}'.format(identifier))\n",
    "\n",
    "        return res\n",
    "\n",
    "    return identifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ResnetBuilder(object):\n",
    "\n",
    "    @staticmethod\n",
    "\n",
    "    def build(input_shape, num_outputs, block_fn, repetitions):\n",
    "\n",
    "        \"\"\"Builds a custom ResNet like architecture.\n",
    "\n",
    "\n",
    "\n",
    "        Args:\n",
    "\n",
    "            input_shape: The input shape in the form (nb_channels, nb_rows, nb_cols)\n",
    "\n",
    "            num_outputs: The number of outputs at final softmax layer\n",
    "\n",
    "            block_fn: The block function to use. This is either `basic_block` or `bottleneck`.\n",
    "\n",
    "                The original paper used basic_block for layers < 50\n",
    "\n",
    "            repetitions: Number of repetitions of various block units.\n",
    "\n",
    "                At each block unit, the number of filters are doubled and the input size is halved\n",
    "\n",
    "\n",
    "\n",
    "        Returns:\n",
    "\n",
    "            The keras `Model`.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        _handle_dim_ordering()\n",
    "\n",
    "        if len(input_shape) != 3:\n",
    "\n",
    "            raise Exception(\"Input shape should be a tuple (nb_channels, nb_rows, nb_cols)\")\n",
    "\n",
    "\n",
    "\n",
    "        # Permute dimension order if necessary\n",
    "\n",
    "        if K.image_dim_ordering() == 'tf':\n",
    "\n",
    "            input_shape = (input_shape[1], input_shape[2], input_shape[0])\n",
    "\n",
    "\n",
    "\n",
    "        # Load function from str if needed.\n",
    "\n",
    "        block_fn = _get_block(block_fn)\n",
    "\n",
    "\n",
    "\n",
    "        input = Input(shape=input_shape)\n",
    "\n",
    "        conv1 = _conv_bn_relu(filters=64, kernel_size=(7, 7), strides=(2, 2))(input)\n",
    "\n",
    "        pool1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(conv1)\n",
    "\n",
    "\n",
    "\n",
    "        block = pool1\n",
    "\n",
    "        filters = 64\n",
    "\n",
    "        for i, r in enumerate(repetitions):\n",
    "\n",
    "            block = _residual_block(block_fn, filters=filters, repetitions=r, is_first_layer=(i == 0))(block)\n",
    "\n",
    "            filters *= 2\n",
    "\n",
    "\n",
    "\n",
    "        # Last activation\n",
    "\n",
    "        block = _bn_relu(block)\n",
    "\n",
    "\n",
    "\n",
    "        # Classifier block\n",
    "\n",
    "        block_shape = K.int_shape(block)\n",
    "\n",
    "        pool2 = AveragePooling2D(pool_size=(block_shape[ROW_AXIS], block_shape[COL_AXIS]),\n",
    "\n",
    "                                 strides=(1, 1))(block)\n",
    "\n",
    "        flatten1 = Flatten()(pool2)\n",
    "\n",
    "        dense = Dense(units=num_outputs, kernel_initializer=\"he_normal\",\n",
    "\n",
    "                      activation=\"softmax\")(flatten1)\n",
    "\n",
    "\n",
    "\n",
    "        model = Model(inputs=input, outputs=dense)\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "\n",
    "    def build_resnet_18(input_shape, num_outputs):\n",
    "\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "\n",
    "    def build_resnet_34(input_shape, num_outputs):\n",
    "\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "\n",
    "    def build_resnet_50(input_shape, num_outputs):\n",
    "\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "\n",
    "    def build_resnet_101(input_shape, num_outputs):\n",
    "\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "\n",
    "    def build_resnet_152(input_shape, num_outputs):\n",
    "\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 8, 36, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
